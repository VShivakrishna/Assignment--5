{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vItzqcpiQlUp",
        "outputId": "55d85124-8e97-4b41-a318-d30a7ff14941"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 100, 100)          1707700   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 128)               117248    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,825,077\n",
            "Trainable params: 1,825,077\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "347/347 [==============================] - 112s 311ms/step - loss: -0.0132 - accuracy: 0.5308 - val_loss: -0.5485 - val_accuracy: 0.6162\n",
            "Epoch 2/5\n",
            "347/347 [==============================] - 109s 315ms/step - loss: -2.6353 - accuracy: 0.6255 - val_loss: 0.1175 - val_accuracy: 0.5268\n",
            "Epoch 3/5\n",
            "347/347 [==============================] - 108s 312ms/step - loss: -1.7941 - accuracy: 0.5716 - val_loss: -1.1887 - val_accuracy: 0.4422\n",
            "Epoch 4/5\n",
            "347/347 [==============================] - 109s 315ms/step - loss: -5.8419 - accuracy: 0.6287 - val_loss: -2.1649 - val_accuracy: 0.5600\n",
            "Epoch 5/5\n",
            "347/347 [==============================] - 108s 311ms/step - loss: -11.5491 - accuracy: 0.6790 - val_loss: -4.9094 - val_accuracy: 0.5741\n",
            "1/1 [==============================] - 0s 292ms/step\n",
            "Predicted sentiment: positive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from keras import models\n",
        "import joblib\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('/content/sample_data/sentiment.csv')\n",
        "\n",
        "# Select the relevant columns for sentiment analysis\n",
        "data = data[['text', 'sentiment']]\n",
        "\n",
        "# Remove any rows with missing values in 'text' or 'sentiment' columns\n",
        "data = data.dropna(subset=['text', 'sentiment'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X = data['text']\n",
        "y = data['sentiment']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to numerical values (0 and 1)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_sequence = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequence = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences to a fixed length for uniformity\n",
        "max_sequence_length = 100  # You can set this based on your dataset and sequence length distribution\n",
        "X_train_padded = pad_sequences(X_train_sequence, maxlen=max_sequence_length)\n",
        "X_test_padded = pad_sequences(X_test_sequence, maxlen=max_sequence_length)\n",
        "\n",
        "# Build the RNN model\n",
        "embedding_dim = 100  # You can adjust this dimension based on the size of your vocabulary and available word embeddings\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Train the RNN model\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "model.fit(X_train_padded, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_padded, y_test))\n",
        "\n",
        "# Save the trained model to disk\n",
        "model_filename = 'sentiment_analysis_rnn_model.h5'\n",
        "model.save(model_filename)\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = models.load_model(model_filename)\n",
        "\n",
        "# Example text for prediction\n",
        "new_text = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing. @realDonaldTrump\"\n",
        "\n",
        "# Tokenize and pad the new text\n",
        "new_text_sequence = tokenizer.texts_to_sequences([new_text])\n",
        "new_text_padded = pad_sequences(new_text_sequence, maxlen=max_sequence_length)\n",
        "\n",
        "# Predict sentiment using the loaded model\n",
        "predicted_sentiment_probs = loaded_model.predict(new_text_padded)\n",
        "\n",
        "# Threshold the probabilities to get the final sentiment label (e.g., 'positive' or 'negative')\n",
        "threshold = 0.5  # You can adjust this threshold as per your requirement\n",
        "\n",
        "predicted_sentiment_label = ['positive' if prob > threshold else 'negative' for prob in predicted_sentiment_probs]\n",
        "\n",
        "print(\"Predicted sentiment:\", predicted_sentiment_label[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4b6oq1RcZey",
        "outputId": "43ed6620-133a-433c-c764-0911db7f91ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-1a0d126e022b>:48: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  model = KerasClassifier(build_fn=create_model, epochs=1, batch_size=16, verbose=2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "388/388 - 34s - loss: 0.8472 - accuracy: 0.6379 - 34s/epoch - 88ms/step\n",
            "194/194 - 2s - loss: 0.7757 - accuracy: 0.6685 - 2s/epoch - 11ms/step\n",
            "388/388 - 32s - loss: 0.8435 - accuracy: 0.6423 - 32s/epoch - 82ms/step\n",
            "194/194 - 2s - loss: 0.7761 - accuracy: 0.6611 - 2s/epoch - 10ms/step\n",
            "388/388 - 30s - loss: 0.8411 - accuracy: 0.6362 - 30s/epoch - 78ms/step\n",
            "194/194 - 2s - loss: 0.7937 - accuracy: 0.6619 - 2s/epoch - 9ms/step\n",
            "388/388 - 50s - loss: 0.8677 - accuracy: 0.6289 - 50s/epoch - 128ms/step\n",
            "194/194 - 3s - loss: 0.8094 - accuracy: 0.6423 - 3s/epoch - 14ms/step\n",
            "388/388 - 56s - loss: 0.8492 - accuracy: 0.6313 - 56s/epoch - 144ms/step\n",
            "194/194 - 3s - loss: 0.7852 - accuracy: 0.6562 - 3s/epoch - 17ms/step\n",
            "388/388 - 53s - loss: 0.8489 - accuracy: 0.6343 - 53s/epoch - 138ms/step\n",
            "194/194 - 4s - loss: 0.7867 - accuracy: 0.6600 - 4s/epoch - 21ms/step\n",
            "388/388 - 67s - loss: 0.8465 - accuracy: 0.6331 - 67s/epoch - 173ms/step\n",
            "194/194 - 4s - loss: 0.8038 - accuracy: 0.6530 - 4s/epoch - 20ms/step\n",
            "388/388 - 69s - loss: 0.8495 - accuracy: 0.6344 - 69s/epoch - 178ms/step\n",
            "194/194 - 4s - loss: 0.8014 - accuracy: 0.6591 - 4s/epoch - 20ms/step\n",
            "388/388 - 68s - loss: 0.8455 - accuracy: 0.6322 - 68s/epoch - 176ms/step\n",
            "194/194 - 4s - loss: 0.8048 - accuracy: 0.6577 - 4s/epoch - 20ms/step\n",
            "388/388 - 34s - loss: 0.8376 - accuracy: 0.6381 - 34s/epoch - 89ms/step\n",
            "194/194 - 2s - loss: 0.7662 - accuracy: 0.6704 - 2s/epoch - 11ms/step\n",
            "388/388 - 33s - loss: 0.8354 - accuracy: 0.6412 - 33s/epoch - 84ms/step\n",
            "194/194 - 2s - loss: 0.7858 - accuracy: 0.6640 - 2s/epoch - 10ms/step\n",
            "388/388 - 36s - loss: 0.8304 - accuracy: 0.6427 - 36s/epoch - 93ms/step\n",
            "194/194 - 2s - loss: 0.7559 - accuracy: 0.6713 - 2s/epoch - 11ms/step\n",
            "388/388 - 61s - loss: 0.8467 - accuracy: 0.6389 - 61s/epoch - 156ms/step\n",
            "194/194 - 3s - loss: 0.7541 - accuracy: 0.6724 - 3s/epoch - 17ms/step\n",
            "388/388 - 58s - loss: 0.8319 - accuracy: 0.6449 - 58s/epoch - 148ms/step\n",
            "194/194 - 4s - loss: 0.7805 - accuracy: 0.6750 - 4s/epoch - 21ms/step\n",
            "388/388 - 60s - loss: 0.8319 - accuracy: 0.6409 - 60s/epoch - 155ms/step\n",
            "194/194 - 3s - loss: 0.7967 - accuracy: 0.6493 - 3s/epoch - 17ms/step\n",
            "388/388 - 73s - loss: 0.8429 - accuracy: 0.6378 - 73s/epoch - 188ms/step\n",
            "194/194 - 4s - loss: 0.7822 - accuracy: 0.6582 - 4s/epoch - 21ms/step\n",
            "388/388 - 72s - loss: 0.8392 - accuracy: 0.6383 - 72s/epoch - 185ms/step\n",
            "194/194 - 4s - loss: 0.7635 - accuracy: 0.6737 - 4s/epoch - 22ms/step\n",
            "388/388 - 72s - loss: 0.8421 - accuracy: 0.6404 - 72s/epoch - 187ms/step\n",
            "194/194 - 5s - loss: 0.7744 - accuracy: 0.6726 - 5s/epoch - 26ms/step\n",
            "388/388 - 43s - loss: 0.8308 - accuracy: 0.6400 - 43s/epoch - 112ms/step\n",
            "194/194 - 3s - loss: 0.7449 - accuracy: 0.6853 - 3s/epoch - 13ms/step\n",
            "388/388 - 42s - loss: 0.8285 - accuracy: 0.6378 - 42s/epoch - 109ms/step\n",
            "194/194 - 4s - loss: 0.7771 - accuracy: 0.6717 - 4s/epoch - 19ms/step\n",
            "388/388 - 41s - loss: 0.8255 - accuracy: 0.6454 - 41s/epoch - 106ms/step\n",
            "194/194 - 4s - loss: 0.7849 - accuracy: 0.6493 - 4s/epoch - 18ms/step\n",
            "388/388 - 67s - loss: 0.8314 - accuracy: 0.6394 - 67s/epoch - 172ms/step\n",
            "194/194 - 5s - loss: 0.7707 - accuracy: 0.6649 - 5s/epoch - 24ms/step\n",
            "388/388 - 66s - loss: 0.8305 - accuracy: 0.6399 - 66s/epoch - 169ms/step\n",
            "194/194 - 4s - loss: 0.7889 - accuracy: 0.6585 - 4s/epoch - 23ms/step\n",
            "388/388 - 66s - loss: 0.8234 - accuracy: 0.6451 - 66s/epoch - 169ms/step\n",
            "194/194 - 5s - loss: 0.7896 - accuracy: 0.6700 - 5s/epoch - 25ms/step\n",
            "388/388 - 82s - loss: 0.8431 - accuracy: 0.6392 - 82s/epoch - 210ms/step\n",
            "194/194 - 7s - loss: 0.7964 - accuracy: 0.6720 - 7s/epoch - 34ms/step\n",
            "388/388 - 89s - loss: 0.8408 - accuracy: 0.6378 - 89s/epoch - 229ms/step\n",
            "194/194 - 6s - loss: 0.7826 - accuracy: 0.6646 - 6s/epoch - 29ms/step\n",
            "388/388 - 97s - loss: 0.8300 - accuracy: 0.6401 - 97s/epoch - 250ms/step\n",
            "194/194 - 6s - loss: 0.7649 - accuracy: 0.6697 - 6s/epoch - 33ms/step\n",
            "581/581 - 128s - loss: 0.8068 - accuracy: 0.6533 - 128s/epoch - 220ms/step\n",
            "Best hyperparameters: {'embed_dim': 256, 'lstm_out': 256}\n",
            "144/144 - 8s - loss: 0.7667 - accuracy: 0.6686 - 8s/epoch - 56ms/step\n",
            "Test Score: 0.7666570544242859\n",
            "Test Accuracy: 0.6686325669288635\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "data = pd.read_csv('/content/sample_data/sentiment.csv')\n",
        "# Keeping only the necessary columns\n",
        "data = data[['text', 'sentiment']]\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "max_fatures = 2000\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "def create_model(embed_dim, lstm_out):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_fatures, embed_dim, input_length=X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "# Create KerasClassifier wrapper for GridSearchCV\n",
        "model = KerasClassifier(build_fn=create_model, epochs=1, batch_size=16, verbose=2)\n",
        "\n",
        "# Define hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'embed_dim': [64, 128, 256],\n",
        "    'lstm_out': [128, 196, 256]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(model, param_grid, cv=3)\n",
        "\n",
        "# Perform the grid search on the training data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best hyperparameters and best model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on the test data\n",
        "score = best_model.model.evaluate(X_test, Y_test, verbose=2)\n",
        "print(score)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}